ä»¥ä¸‹ãŒã€RT-DETR (PyTorch) ã‚’ç”¨ã„ãŸå­¦ç¿’ãƒ»æ¨è«–ã‚’è¡Œã†ãŸã‚ã®æœ€å°æ§‹æˆã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆã¨ã€å¿…è¦ãªã‚³ãƒ¼ãƒ‰ãƒ»å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®ä¸€è¦§ã§ã™ï¼š

â¸»

ğŸ“ æœ€å°æ§‹æˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

rtdetr_minimal/
â”‚   train.py              # PyTorchã§ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   infer.py              # æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   model.py              # RT-DETRãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
â”‚   dataset.py            # COCOå½¢å¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
â”‚   requirements.txt      # ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
â”‚
â””â”€â”€â”€data/
    â””â”€â”€â”€images/           # å…¥åŠ›ç”»åƒï¼ˆä¾‹: image1.jpg, image2.jpgï¼‰
    â””â”€â”€â”€annotations/      # COCOå½¢å¼ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: instances_train.jsonï¼‰


â¸»

ğŸ§± requirements.txt

torch>=2.0.0
torchvision>=0.15.0
pycocotools
opencv-python
tqdm


â¸»

ğŸ“¦ model.py â€“ ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆhubä½¿ç”¨ï¼‰

import torch

def get_model(pretrained=True, num_classes=91):
    model = torch.hub.load('lyuwenyu/RT-DETR', 'rtdetr_r50', pretrained=pretrained)
    if num_classes != 91:
        model.head.class_embed = torch.nn.Linear(model.head.class_embed.in_features, num_classes)
    return model


â¸»

ğŸ“ dataset.py â€“ COCOå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼

import torch
from torchvision.datasets import CocoDetection
import torchvision.transforms as T
from PIL import Image

class CocoDataset(CocoDetection):
    def __init__(self, img_dir, ann_file):
        super().__init__(img_dir, ann_file)
        self.transform = T.Compose([T.ToTensor()])

    def __getitem__(self, idx):
        img, anns = super().__getitem__(idx)
        img = self.transform(img)
        boxes = [ann['bbox'] for ann in anns]
        labels = [ann['category_id'] for ann in anns]
        target = {
            'boxes': torch.tensor(boxes, dtype=torch.float32),
            'labels': torch.tensor(labels, dtype=torch.int64)
        }
        return img, target


â¸»

ğŸ‹ï¸â€â™€ï¸ train.py â€“ å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import torch
from torch.utils.data import DataLoader
from dataset import CocoDataset
from model import get_model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = get_model(num_classes=2).to(device)

dataset = CocoDataset("data/images", "data/annotations/instances_train.json")
dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

model.train()
for epoch in range(10):
    for images, targets in dataloader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        loss = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")

torch.save(model.state_dict(), "rtdetr_minimal.pth")


â¸»

ğŸ” infer.py â€“ æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import torch
import cv2
from model import get_model
import torchvision.transforms as T

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = get_model(num_classes=2, pretrained=False)
model.load_state_dict(torch.load("rtdetr_minimal.pth", map_location=device))
model.to(device).eval()

def infer(image_path):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    transform = T.ToTensor()
    input_tensor = transform(img_rgb).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(input_tensor)[0]

    for box, label, score in zip(output['boxes'], output['labels'], output['scores']):
        if score > 0.5:
            box = box.int().cpu().numpy()
            cv2.rectangle(img, tuple(box[:2]), tuple(box[2:]), (0, 255, 0), 2)
            cv2.putText(img, f"{label.item()}:{score:.2f}", (box[0], box[1]-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)

    cv2.imwrite("inferred.jpg", img)

infer("data/images/image1.jpg")


â¸»

ğŸ“ å­¦ç¿’ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆdata/annotations/instances_train.jsonï¼‰

{
  "images": [
    {"id": 1, "file_name": "image1.jpg", "height": 480, "width": 640}
  ],
  "annotations": [
    {"id": 1, "image_id": 1, "category_id": 1, "bbox": [100, 150, 50, 80], "area": 4000, "iscrowd": 0}
  ],
  "categories": [
    {"id": 1, "name": "object"}
  ]
}


â¸»
